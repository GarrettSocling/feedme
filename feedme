#!/usr/bin/env python

# feedme: read RSS/Atom feeds and convert to Plucker files.
# Copyright 2009,2011 Akkana Peck <akkana@shallowsky.com>
# Based on feedread, Copyright (C) 2009 Benjamin M. A'Lee <bma@subvert.org.uk>
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details:
# <http://www.gnu.org/licenses/>.

# Goals for feedme 0.9: use real HTML parsing, not rexexp; add image fetching

#
# Important TODO:
# - Figure out why we get repeat stories on sites like BBC.

VersionString = "FeedMe 0.9"

import cPickle
import time
import os, sys
import re
#import types
import shutil
import traceback

import feedparser
import urllib2
import socket
import posixpath

# We now use a separate file for the parsing and such:
import feedmeparser

has_ununicode=True
try :
    import ununicode
except ImportError, e:
    has_ununicode=False

#
# Clean up old feed directories
#
def clean_up(config) :
    try :
        days = int(config.get('DEFAULT', 'save_days'))
        feeddir = config.get('DEFAULT', 'dir')
        feeddir = feedmeparser.sub_tilde(feeddir)
    except :
        print >>sys.stderr, \
            "Error trying to get save_days and feed dir; can't clean up"
        return

    print >>sys.stderr, "Cleaning up anything older than", \
        days, "days from", feeddir

    now = time.time()
    for dir in os.listdir(feeddir) :
        d = os.path.join(feeddir, dir)

        # Only clean up directories, not files sitting in the feeds dir.
        if not os.path.isdir(d):
            continue

        try :
            howold = (now - os.path.getctime(d)) / 60 / 60 / 24
            if howold > days :
                print >>sys.stderr, "Deleting", d
                if os.path.isdir(d) :
                    shutil.rmtree(d)
                else :
                    os.unlink(d)
        except Exception, e :
            print "Couldn't unlink", d, str(e)

##################################################################
# OUTPUT GENERATING FUNCTIONS
# Define functions for each output format you need to support.
#

def run_conversion_cmd(appargs) :
    if True or verbose :
        cmd = " ".join(appargs)
        print >>sys.stderr, "Running:", cmd
        sys.stdout.flush()

    retval = os.spawnvp(os.P_WAIT, appargs[0], appargs)
    #retval = os.system(cmd)
    if retval != 0 :
        raise OSError(retval, "Couldn't run: " + ' '.join(appargs))

#
# Generate a Plucker file
#
def make_plucker_file(indexfile, feedname, levels, ascii) :
    day = time.strftime("%a")
    docname = day + " " + feedname
    cleanfilename = day + "_" + feedname.replace(" ", "_")

    # Make sure the plucker directory exists:
    pluckerdir = os.path.join(feedmeparser.sub_tilde("~/.plucker"), "feedme")
    if not os.path.exists(pluckerdir) :
        os.makedirs(pluckerdir)

    # Run plucker. This should eventually be configurable --
    # but how, with arguments like these?

    # Plucker mysteriously creates unbeamable files if the
    # document name has a colons in it.
    # So use the less pretty but safer underscored docname.
    #docname = cleanfilename
    appargs = [ "plucker-build", "-N", docname,
                "-f", os.path.join("feedme", cleanfilename),
                "--stayonhost", "--noimages",
                "--maxdepth", str(levels),
                "--zlib-compression", "--beamable",
                "-H", "file://" + indexfile ]
    if not ascii :
        appargs.append("--charset=utf-8")

    run_conversion_cmd(appargs)

#
# http://calibre-ebook.com/user_manual/conversion.html
#
def make_calibre_file(indexfile, feedname, extension, levels, ascii,
                      author, flags) :
    day = time.strftime("%a")
    # Prepend daynum to the filename because fbreader can only sort by filename
    #daynum = time.strftime("%w")
    cleanfilename = day + "_" + feedname.replace(" ", "_")
    outdir = os.path.join(config.get('DEFAULT', 'dir'), extension[1:])
    if not os.access(outdir, os.W_OK) :
        os.makedirs(outdir)

    appargs = [ "ebook-convert",
                indexfile,
                #os.path.join(feedmeparser.sub_tilde("~/feeds"),
                #             cleanfilename + extension),
                # directory should be configurable too, probably
                os.path.join(outdir, cleanfilename + extension),
                "--authors", author ]
    for flag in flags :
        appargs.append(flag)
    if True or verbose :
        cmd = " ".join(appargs)
        print >>sys.stderr, "Running:", cmd
        sys.stdout.flush()

    run_conversion_cmd(appargs)

#
# Generate a fictionbook2 file
#
def make_fb2_file(indexfile, feedname, levels, ascii) :
    make_calibre_file(indexfile, feedname, ".fb2", levels, ascii,
                      "feedme", flags = [ "--disable-font-rescaling" ] )

#
# Generate an ePub file
# http://calibre-ebook.com/user_manual/cli/ebook-convert-3.html#html-input-to-epub-output
#
def make_epub_file(indexfile, feedname, levels, ascii) :
    make_calibre_file(indexfile, feedname, ".epub", levels, ascii,
                      time.strftime("%m-%d %a") + " feeds",
                      flags = [ '--no-default-epub-cover',
                                '--dont-split-on-page-breaks' ])

# END OUTPUT GENERATING FUNCTIONS
##################################################################

##################################################################
# MsgLog: Print messages and also batch them up to print at the end:
#
class MsgLog :
    def __init__(self) :
        self.msgstr = ""
        self.errstr = ""

    def msg(self, s) :
        self.msgstr += "\n" + s
        print >>sys.stderr, "MESSAGE:", s.encode('ascii', 'backslashreplace')

    def warn(self, s) :
        self.msgstr += "\n" + s
        print  >>sys.stderr,"WARNING:", s.encode('ascii', 'backslashreplace')

    def err(self, s) :
        self.errstr += "\n" + s
        print  >>sys.stderr,"ERROR:", s.encode('ascii', 'backslashreplace')
        #traceback.print_stack()

    def get_msgs(self) :
        return self.msgstr

    def get_errs(self) :
        return self.errstr

# file-like class that can optionally send output to a log file. Inspired by
# http://www.redmountainsw.com/wordpress/archives/python-subclassing-file-types
# and with help from KirkMcDonald.
class tee() :
    def __init__(self, _fd1, _fd2) :
        self.fd1 = _fd1
        self.fd2 = _fd2

    def __del__(self) :
        if self.fd1 != sys.stdout and self.fd1 != sys.stderr :
            self.fd1.close()
        if self.fd2 != sys.stdout and self.fd2 != sys.stderr :
            self.fd2.close()

    def write(self, text) :
        if isinstance(text, unicode):
            text = text.encode('utf-8')
        self.fd1.write(text)
        self.fd2.write(text)

    def flush(self) :
        self.fd1.flush()
        self.fd2.flush()

#
# Ctrl-C Interrupt handler: prompt for what to do.
#
def handle_keyboard_interrupt(msg) :
    # os.isatty() doesn't work, so:
    if not hasattr(sys.stdin, "isatty") :
        print "Interrupt, and not running interactively. Exiting."
        sys.exit(1)

    response = raw_input(msg)
    if response == '' :
        return '\0'
    if response[0] == 'q' :
        sys.exit(1)
    return response[0]

def falls_between(when, time1, time2) :
    """Does a given day-of-week or day-of-month fall between
       the two given times? It is presumed that time1 <= time2.
       If when == "Tue", did we cross a tuesday getting from time1 to time2?
       If when == 15, did we cross the 15th of a month?
       If when == none, return True.
       If when matches time2, return True.
    """
    print >>sys.stderr, "Does", when, "fall between", time1, "and", time2, "?"
    if not when or type(when) is str and len(when) <= 0 :
        return True

    # We need both times both in seconds since epoch and in struct_time:
    def both_time_types(t) :
        """Given a time that might be either seconds since epoch or struct_time,
           return a tuple of (seconds, struct_time).
        """
        if type(t) is time.struct_time :
            return time.mktime(t), t
        elif type(t) is int or type(t) is float :
            return t, time.localtime(t)
        else : raise ValueError("%s not int or struct_time" % str(t))

    (t1, st1) = both_time_types(time1)
    print >>sys.stderr, "Time1:", t1, st1
    (t2, st2) = both_time_types(time2)
    print >>sys.stderr, "Time2:", t2, st2

    daysdiff = (t2 - t1) / 60. / 60. / 24.
    print >>sys.stderr, "daysdiff is", daysdiff
    if daysdiff < 0 :
        msglog.err("daysdiff < 0!!! " + str(daysdiff))

    # Is it a day of the month?
    try :
        day_of_month = int(when)
        print >>sys.stderr, "It's a day of the month:", day_of_month,
        print >>sys.stderr, "compared to", st1.tm_mday, "and", st2.tm_mday

        # It is a day of the month! How many days in between the two dates?
        if daysdiff > 31 :
            return True

        # Now we know the two dates differ by less than a month.
        # Are time1 and time2 both in the same month? Then it's easy.
        if st1.tm_mon == st2.tm_mon :
            return st1.tm_mday <= day_of_month and st2.tm_mday >= day_of_month

        # Else time1 is the month prior to time2, so:
        return st1.tm_mday < day_of_month or day_of_month <= st2.tm_mday

    except ValueError :  # Not an integer, probably a string.
        pass

    if type(when) is not str :
        raise ValueError("%s must be a string or integer" % when)

    # Okay, not a day of the month. Is it a day of the week?
    # We have to start with Monday because struct_time.tm_wday does.
    weekdays = [ 'mo', 'tu', 'we', 'th', 'fr', 'sa', 'su' ]
    if len(when) < 2 :
        raise ValueError("%s too short: days must have at least 2 chars" % when)

    when = when[0:2].lower()
    if when not in weekdays :
        raise ValueError("%s is a string but not a day" % when)

    # Whew -- we know it's a day of the week.

    # Has more than a week passed? Then it encompasses all weekdays.
    if daysdiff > 7 :
        print >>sys.stderr, "More than a week has passed"
        return True

    day_of_week = weekdays.index(when)
    print >>sys.stderr, when, "is weekday #", day_of_week,
    print >>sys.stderr, "compared to", st1.tm_wday, "and", st2.tm_wday
    print >>sys.stderr, "Will return", (st2.tm_wday - day_of_week) % 7, '<', daysdiff
    return (st2.tm_wday - day_of_week) % 7 < daysdiff

#
# Get a single feed
#
def get_feed(feedname, config, cache, cachefile, last_time, msglog) :
    """Fetch a single feed"""
    # Mandatory arguments:
    try :
        sitefeedurl = config.get(feedname, 'url')
        feeddir = config.get(feedname, 'dir')
    except :
        msglog.err("Error reading feedme.conf entry for: " + feedname)
        return

    verbose = (config.get(feedname, 'verbose').lower() == 'true')
    levels = int(config.get(feedname, 'levels'))

    feeddir = feedmeparser.sub_tilde(feeddir)
    feeddir = os.path.join(feeddir, time.strftime("%m-%d-%a"))

    formats = config.get(feedname, 'formats').split(',')
    encoding = config.get(feedname, 'encoding')
    ascii = config.getboolean(feedname, 'ascii')
    skip_links = config.getboolean(feedname, 'skip_links')
    skip_link_pat = feedmeparser.get_config_multiline(config,
                                                      feedname,
                                                      'skip_link_pat')

    # Is this a feed we should only check occasionally?
    """Does this feed specify only gathering at certain times?
       If so, has such a time passed since the last time the
       cache file was written?
    """
    when = config.get(feedname, "when")
    if when and when != '' and last_time :
        if not falls_between(when, last_time, time.localtime()) :
            print >>sys.stderr, "Skipping", feedname, "-- not", when
            return
        print >>sys.stderr, "Yes, it's time to feed:", when

    #encoding = config.get(feedname, 'encoding')

    print >>sys.stderr, "\n============\nfeedname:", feedname
    # Use underscores rather than spaces in the filename.
    feedfile = feedname.replace(" ", "_")
    # Also, make sure there are no colons (illegal in filenames):
    feedfile = feedfile.replace(":", "")
    print >>sys.stderr, "feedfile:", feedfile
    outdir = os.path.join(feeddir,  feedfile)
    print >>sys.stderr, "outdir:", outdir

    if cache == None :
        nocache = True
    else :
        nocache = (config.get(feedname, 'nocache') == 'true')
    if verbose and nocache :
        msglog.msg(feedname + ": Ignoring cache")

    global VersionString
    downloaded_string ="\n<hr><i>(Downloaded by " + VersionString + ")</i>\n"

    # feedparser doesn't understand file:// URLs, so translate those
    # to a local file:
    if sitefeedurl.startswith('file://'):
        sitefeedurl = sitefeedurl[7:]

    # feedparser.parse() can throw unexplained errors like
    # "xml.sax._exceptions.SAXException: Read failed (no details available)"
    # which will kill our whole process, so guard against that.
    # Sadly, feedparser usually doesn't give any details about what went wrong.
    socket.setdefaulttimeout(100)
    try :
        print "Running: feedparser.parse(", sitefeedurl, ")"
        feed = feedparser.parse(sitefeedurl)

    # except xml.sax._exceptions.SAXException, e :
    except Exception, e :
        print "Couldn't parse feed: URL:", sitefeedurl
        print str(e)
        traceback.print_stack()
        return

    # feedparser has no error return! One way is to check len(feed.feed).
    if len(feed.feed) == 0 :
        msglog.err("Can't read " + sitefeedurl)
        return
    # XXX Sometimes feeds die a few lines later getting feed.feed.title.
    # Here's a braindead guard against it -- but why isn't this
    # whole clause inside a try? It should be.
    if not 'title' in feed.feed :
        msglog.msg(sitefeedurl + " lacks a title!")
        feed.feed.title = '[' + feedname + ']'
        #return

    if not nocache :
        if sitefeedurl not in cache:
            cache[sitefeedurl] = []
        feedcache = cache[sitefeedurl]
        newfeedcache = []

    # suburls: mapping of URLs we've encountered to local URLs.
    # Any anchors (#anchor) will be discarded.
    # This is for sites like WorldWideWords that make many links
    # to the same page.
    suburls = []

    # indexstr is the contents of the index.html file.
    # Kept as a string until we know whether there are new, non-cached
    # stories so it's worth updating the copy on disk.
    # The stylesheet is for FeedViewer and shouldn't bother plucker etc.
    day = time.strftime("%a")
    indexstr = u"""<html>\n<head>
<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">
<title>%s: %s</title>
<link rel="stylesheet" type="text/css" title="Feeds" href="../../feeds.css"/>
</head>

<body>\n<h1>%s: %s: %s</h1>
\n""" % (day, feedname, day, feedname, feed.feed.title)

    if verbose:
        print >>sys.stderr, "********* Reading", sitefeedurl

    # A pattern to tell the user how to get to the next story: >->
    # We also might want to remove that pattern later, in case
    # a story wasn't successfully downloaded -- so make a
    # regexp that can match it.
    next_item_string =  '<br>\n<center><i><a href=\"#%d\">&gt;-&gt;</a></i></center>\n<br>\n'
    next_item_pattern = '<br>\n<center><i><a href=\"#[0-9]+\">&gt;-&gt;</a></i></center>\n<br>\n'

    # We'll increment itemnum as soon as we start showing entries,
    # so start it negative so anchor links will start at zero.
    itemnum = -1
    for item in feed.entries :
        try :
            #
            # Get the list of links (href) and a (hopefully) unique ID:
            # XXX Is href[] ever even used? Is this clause obsolete?
            #
            if 'links' in item :
                href = [i['href'].encode('utf-8') \
                            for i in item.links if i['rel'] == 'alternate']
            else:
                href = []

            if not 'id' in item :
                if len(href) > 0 :
                    item.id = href[0]
                    if verbose :
                        msglog.msg("Using URL " + href[0] + " for ID.")
                else:
                    if verbose :
                        msglog.msg("Item in " + href[0] + " had no ID or URL.")
                    next  # or return?

            # Does the link match a pattern we're skipping?
            if skip_link_pat :
                skipping = False
                for spat in skip_link_pat :
                    if re.search(spat, item.link) :
                        if verbose :
                            print >>sys.stderr, "Skipping", item.link, \
                                "because it matches", spat
                        skipping = True
                        break
                if skipping :
                    continue

            # Filter out file types known not to work
            # XXX Only mp3 for now. Obviously, make this more general.
            # Wish we could do this using the server's type rather than
            # file extension!
            if item.link.endswith("mp3") :
                print >>sys.stderr, "Filtering out mp3 link", item.link
                continue

            # Make sure ids don't have named anchors appended:
            anchor_index = item.id.rfind('#')
            if anchor_index >= 0 :
                anchor = item.id[anchor_index:]
                item.id = item.id[0:anchor_index]
            else :
                anchor = ""

            # See if we've already seen this page:
            try :
                pagenum = suburls.index(item.id)
                # We've already seen a link to this URL. It's probably
                # a link to a different named anchor within the same file.
            except ValueError :
                # Haven't seen it before. But is it in the cache already?
                if not nocache :
                    # We want it in the cache, whether it's new or not:
                    newfeedcache.append(item.id)
                    if item.id in feedcache:
                        if verbose :
                            msglog.msg(item.id + " already cached -- skipping")
                        continue

                # Add it to the cache and suburls.
                suburls.append(item.id)
                pagenum = len(suburls) - 1

            itemnum += 1
            if verbose :
                print >>sys.stderr, "\nItem:", item.title.encode('utf-8',
                                                                 'replace')

            # Now itemnum is the number of the entry on the index page;
            # pagenum is the html file of the subentry, e.g. 3.html.

            # Make the parent directory if we haven't already
            if not os.access(outdir, os.W_OK) :
                if verbose :
                    print >>sys.stderr, "Making", outdir
                os.makedirs(outdir)

            if 'author' in item :
                author = item.author
            else :
                author = None

            #
            # Follow the link and make a file for it:
            #
            if levels > 1 :        # Normal multi-level site
                try :    # Try to trap keyboard interrupts, + others
                    # For the sub-pages, we're getting HTML, not RSS.
                    # Nobody seems to have RSS pointing to RSS.
                    parser = feedmeparser.FeedmeHTMLParser(config, feedname)
                    fnam = str(pagenum) + ".html"
                    parser.fetch_url(item.link,
                                     outdir, fnam,
                                     item.title, author,
                      "<center><a href=\"%d.html\">&gt;-%d-&gt;</a></center>" \
                                         % (itemnum, itemnum))

                except feedmeparser.NoContentError as e :
                    # fetch_url didn't manage to get the page or write a file.
                    # So don't increment pagenum or itemnum for the next story.
                    msglog.warn("Didn't find any content on " + item.link
                                + ": " + str(e))
                    # It is so annoying needing to repeat these
                    # lines every time! Isn't there a way I can define
                    # a subfunction that knows about this function's
                    # local variables?
                    itemnum -= 1
                    #pagenum -= 1
                    suburls.remove(item.id)

                    # Include a note in the indexstr
                    indexstr += '<p>No content for <a href="%s">%s</a>\n' \
                                % (item.link, item.title)
                    continue

                # Catch timeouts.
                # If we get a timeout on a story,
                # we should assume the whole site has gone down,
                # and skip over the rest of the site.
                # In Python 2.6, instead of raising socket.timeout
                # a timeout will raise urllib2.URLerror with
                # e.reason set to socket.timeout. 
                except socket.timeout, e:
                    errmsg = "Socket.timeout error on title "
                    errmsg += item.title.encode('utf-8', 'replace')
                    errmsg += "\n"
                    errmsg += "Breaking -- hopefully we'll write index.html"
                    msglog.err(errmsg)
                    if config.get(feedname, 'continue_on_timeout') == 'true':
                        continue
                    break
 
                # Handle timeouts in Python 2.6
                except urllib2.URLError, e:
                    if isinstance(e.reason, socket.timeout):
                        errmsg = "URLError Socket.timeout on title "
                        errmsg += item.title.encode('utf-8', 'replace')
                        errmsg += "\n"
                        errmsg += "Breaking -- hopefully we'll write index.html"
                        msglog.err(errmsg)
                        indexstr += "<p>" + errmsg
                        if config.get(feedname,
                                      'continue_on_timeout') == 'true':
                            continue
                        break

                    # Some other type of URLError.
                    errmsg = "URLError on " + item.link + "\n"
                    errmsg += str(e) + '<br>\n'
                    msglog.err(errmsg)
                    indexstr += "<p><b>" + errmsg + "</b>"
                    continue

                except KeyboardInterrupt :
                    response = handle_keyboard_interrupt("""
*** Caught keyboard interrupt reading a story! ***\n
Options:
q: Quit
c: Continue trying to read this story
s: Skip to next story
n: Skip to next site

Which (default = s): """)
                    if response[0] == 'n' :      # next site
                        # XXX We should write an index.html here
                        # with anything we've gotten so far.
                        # Ideally we'd break out of the
                        # for item in feed.entries : loop.
                        # Wonder if there's a way to do that in python?
                        # Failing that, and hoping it's the only
                        # enclosing loop:
                        print "Breaking -- hopefully we'll write an index.html"
                        break
                        #return
                    elif response[0] != 'c' :    # next story (default)
                        continue
                    # If the response was 'c', we continue and just
                    # ignore the interrupt.

                except (IOError, urllib2.HTTPError) as e :
                    # Collect info about what went wrong:
                    errmsg = "Couldn't read " + item.link + "\n"
                    #errmsg += "Title: " + item.title.encode('utf-8', 'replace')
                    if verbose :
                        #errmsg += "Item summary was:\n------\n"
                        #errmsg += item.summary + "\n------\n"
                        errmsg += str(e) + '<br>\n'
                        #errmsg += str(sys.exc_info()[0]) + '<br>\n'
                        #errmsg += str(sys.exc_info()[1]) + '<br>\n'
                        #errmsg += traceback.format_exc(sys.exc_info()[2])

                    if verbose :
                        print >>sys.stderr, "=============="
                    msglog.err("IO or HTTP error: " + errmsg)
                    if verbose :
                        print >>sys.stderr, "=============="

                    itemnum -= 1
                    suburls.remove(item.id)

                    #raise  # so this entry won't get stored or cached

                    continue   # Move on to next story

                except ValueError, e :
                    # urllib2 is supposed to throw a urllib2.URLError for
                    # "unknown url type", but in practice it throws ValueError.
                    # See this e.g. for doubleclick ad links in the latimes
                    # that have no spec, e.g. //ad.doubleclick.net/...
                    # Unfortunately it seems to happen in other cases too,
                    # so there's no way to separate out the urllib2 ones
                    # except by string: str(sys.exc_info()[1]) starts with
                    # "unknown url type:"
                    errmsg = "ValueError on title "
                    errmsg += item.title.encode('utf-8', 'replace')
                    errmsg += "\n"
                    # print >>sys.stderr, errmsg
                    # msglog.err will print it, no need to print it again.
                    if str(sys.exc_info()[1]).startswith("unknown url type:") :
                        # Don't show stack trace for unknown URL types,
                        # since it's a known error.
                        errmsg += str(sys.exc_info()[1]) + " - couldn't load\n"
                        msglog.warn(errmsg)
                    else :
                        errmsg += "ValueError on url " + item.link + "\n"
                        errmsg += traceback.format_exc(sys.exc_info()[2])
                        msglog.err(errmsg)

                    itemnum -= 1
                    suburls.remove(item.id)
                    continue

                except Exception as e :
                    # An unknown error, so report it complete with traceback.
                    errmsg = "Unknown error reading " + item.link + "\n"
                    errmsg += "Title: " + item.title.encode('utf-8', 'replace')
                    if verbose :
                        errmsg += "\nItem summary was:\n------\n"
                        errmsg += item.summary + "\n------\n"
                        errmsg += str(e) + '<br>\n'
                        errmsg += str(sys.exc_info()[0]) + '<br>\n'
                        errmsg += str(sys.exc_info()[1]) + '<br>\n'
                        errmsg += traceback.format_exc(sys.exc_info()[2])

                    if verbose :
                        print >>sys.stderr, "=============="
                    msglog.err("Unknown error: " + errmsg)
                    if verbose :
                        print >>sys.stderr, "=============="

                    # Are we sure we didn't get anything?
                    # Should we decrement itemnum, etc. ?
                    continue   # Move on to next story, ensure we get index

            # Done with if levels > 1 clause
            
            if not 'published_parsed' in item:
                if 'updated_parsed' in item:
                    item.published_parsed = item.updated_parsed
                else:
                    item.published_parsed = time.gmtime()

            # Plucker named anchors don't work unless preceded by a <p>
     # http://www.mail-archive.com/plucker-list@rubberchicken.org/msg07314.html
            indexstr += "<p><a name=\"%d\">&nbsp;</a>" % itemnum

            # Make sure the link is at least some minimum width.
            # This is for viewers that have special areas defined on the
            # screen, e.g. areas for paging up/down or adjusting brightness.
            minwidth = config.getint(feedname, 'min_width')
            if len(item.title) < minwidth :
                #item.title += '&nbsp;' * (minwidth - len(item.title) - 2) + '__'
                item.title += '. ' * (minwidth - len(item.title)) + '__'

            if levels > 1 :
                itemlink = '<a href=\"' + fnam + anchor + '\">'
                indexstr += itemlink + '<b>' + item.title + '</b></a>\n'
            else :
                # For a single-level site, don't put links over each entry.
                if skip_links :
                    itemlink = None
                    indexstr += "\n<b>" + item.title + "</b>\n"
                else :
                    itemlink = '<a href=\"' + item.link + '\">'
                    indexstr += "\n" + itemlink + item.title + "</a>\n"

            # Under the title, add a link to jump to the next entry.
            # If it's the last entry, we'll change it to "[end]" later.
            indexstr += next_item_string % (itemnum+1)

            # Add either the content or the summary:
            if 'summary_detail' in item:
                content = item.summary_detail.value + "\n"
            elif 'content' in item :
                content = item.content[0].value + "\n"
            else :
                content = "[No content]"

            # There's an increasing trend to load up RSS pages with images.
            # Try to remove them, as well as any links that contain
            # only an image.
            if config.getboolean(feedname, 'skip_images') :
                content = re.sub('<a [^>]*href=.*> *<img .*?></a>', '', content)
                content = re.sub('<img .*?>', '', content)
            # But if we're not skipping images, then we need to rewrite
            # image URLs to the local URLs we would have created in
            # feedmeparser.parse().
            else:
                if content.strip():
                    content = parser.rewrite_images(content)

            # Try to get rid of embedded links if skip_links is true:
            if skip_links :
                content = re.sub('<a href=.*>(.*?)</a>', '\\1', content)
            # If we're keeping links, don't keep empty ones:
            else :
                content = re.sub('<a  [^>]*href=.*> *</a>', '', content)

            # Skip any text specified in index_skip_pat.
            # Some sites (*cough* Pro Publica *cough*) do weird things
            # like putting huge <style> sections in the RSS.
            index_skip_pats = feedmeparser.get_config_multiline(config,
                                                      feedname,
                                                      'index_skip_pat')
            for pat in index_skip_pats :
                content = re.sub(pat, '', content)
                author = re.sub(pat, '', author)

            indexstr += content

            if author :
                indexstr += "\n<br><i>By: " + author + "</i><br>"

            # After the content, add another link to the title,
            # in case the user wants to click through after reading
            # the content:
            sublen = 16
            if len(item.title) > sublen :
                # Truncate the title to sublen characters, and
                # temove any HTML tags, otherwise we'll likely have
                # tags like <i> that open but don't close
                short_title = re.sub('<.*?>', '', item.title[0:sublen]) \
                    + "..."

            else :
                short_title = item.title
            if itemlink :
                indexstr += "\n<br>[[" + itemlink + short_title + "</a>]]\n\n"

        # If there was an error parsing this entry, we won't save
        # a file so decrement the itemnum and loop to the next entry.
        except KeyboardInterrupt :
            sys.stderr.flush()
            response = handle_keyboard_interrupt("""
*** Caught keyboard interrupt while finishing a site! ***\n
Options:
q: Quit
c: Continue trying to finish this site
n: Skip to next site

Which (default = n): """)
            if response[0] == 'c' :
                continue
            if response[0] == 'q' :
                sys.exit(1)
            # Default is to skip to the next site:
            return
        except Exception, e :    # probably an HTTPError, bad URL
            itemnum -= 1
            if verbose :
                print >>sys.stderr, "Skipping item",
                if 'link' in item :
                    print >>sys.stderr, item.link.encode('utf-8')
                else :
                    print >>sys.stderr, "item has no link! item =", item
                print >>sys.stderr, "error was", str(e).encode('utf-8')

                print >>sys.stderr, str(sys.exc_info()[0])
                print >>sys.stderr, str(sys.exc_info()[1])
                print >>sys.stderr, traceback.format_exc(sys.exc_info()[2])

    # Only write the index.html file if there was content that
    # wasn't already in the cache.
    if itemnum >= 0 :
        # If the RSS page ended with a story we didn't include,
        # either because it's old or because it had no content we could show,
        # we'll have an extra ">->" line. Try to remove it.
        # (Ugh, python re has no cleaner way of getting the last match.)
        m = None
        for m in re.finditer(next_item_pattern, indexstr) :
            pass
        if m:
            print >>sys.stderr, "Removing final next-item pattern"
            indexstr = indexstr[:m.start()] \
                + "<br>\n<center><i>[end]</i></center>\n<br>\n" \
                + indexstr[m.end():]
        
        indexfile = os.path.join(outdir, "index.html")
        if verbose :
            print  >>sys.stderr, "Writing", indexfile
        index = open(indexfile, "w")
        if ascii :
            index.write(feedmeparser.output_encode(indexstr, 'ascii'))
        else :
            index.write(feedmeparser.output_encode(indexstr, encoding))

        # Before the downloaded string, insert a final named anchor.
        # On some sites we get a bug where we accidentally write a >>
        # when there are really no further stories. So give it a
        # place to go. Though if the removal of the final >->
        # succeeded, this should no longer be needed.
        index.write("<p><a name=\"%d\">&nbsp;</a>\n" % (itemnum+1))

        index.write(downloaded_string)
        index.write("\n</body>\n</html>\n")
        index.close()

        ####################################################
        # Generate the output files
        #
        if 'plucker' in formats :
            make_plucker_file(indexfile, feedname, levels, ascii)
        if 'fb2' in formats :
            make_fb2_file(indexfile, feedname, levels, ascii)
        if 'epub' in formats :
            make_epub_file(indexfile, feedname, levels, ascii)

        #
        # All done. Update the cache file.
        #
        if not nocache :
            if verbose :
                print >>sys.stderr, feedname, ": Updating cache file"
            # Dump the new cache, not the old one:
            # XXX Find out how long this is taking.
            # XXX Should we split the cache into per site?
            t = time.time()
            cache[sitefeedurl] = newfeedcache
            cPickle.dump(cache, open(cachefile, 'w'))
            print >>sys.stderr, "Writing cache took", time.time() - t, "seconds"
        elif verbose :
            print >>sys.stderr, feedname, ": Not updating cache file"

    else :
        print >>sys.stderr, feedname, ": no new content"

        # We may have made the directory. If so, remove it:
        # if there's no index file then there's no way to access anything there.
        if os.path.exists(outdir) :
            print >>sys.stderr, "Removing directory", outdir
            shutil.rmtree(outdir)

#
# Find the cache file and load it, but don't parse yet
#
def init_cache() :
    #
    # Load the cache file
    #
    if 'XDG_CACHE_HOME' in os.environ:
        cachefile = os.path.join(os.environ['XDG_CACHE_HOME'],
                                 'feedme', 'feedme.dat')
    else:
        cachefile = os.path.join(feedmeparser.sub_tilde('~/.cache'),
                                 'feedme', 'feedme.dat')

    if not os.path.exists(cachefile) :
        dirname = os.path.dirname(cachefile)
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        cache = {}
    elif not os.access(cachefile, os.W_OK) :
        print >>sys.stderr, "Error: can't write cache file", cachefile
        sys.exit(1)
    else :
        # Make a backup of the cache file, in case something goes wrong:
        shutil.copy2(cachefile, cachefile + ".bak")
        cache = cPickle.load(open(cachefile))

    return cache, cachefile

#
# Main -- read the config file and loop over sites.
#
if __name__ == '__main__':
    from optparse import OptionParser

    usage = """Usage: %prog [site ...]
If no site is specified, feedme will update all the feeds in
~/.config/feedme.conf."""
    LongVersion = VersionString + "0.8: an RSS feed reader.\n\
Copyright 2011 by Akkana Peck; share and enjoy under the GPL v2 or later."

    optparser = OptionParser(usage=usage, version=LongVersion)
    optparser.add_option("-n", "--nocache",
                         action="store_true", dest="nocache",
                         help="Don't consult the cache, or update it")
    optparser.add_option("-s", "--show-sites",
                         action="store_true", dest="show_sites",
                         help="Show available sites")
    optparser.add_option("-l", "--log", metavar="logfile",
                         action="store", dest="log_file_name",
                         help="Save output to a log file")
    (options, args) = optparser.parse_args()

    config = feedmeparser.read_config_file()

    msglog = MsgLog()

    sections = config.sections()

    if options.show_sites :
        for feedname in sections :
            print feedname
        sys.exit(0)

    if options.nocache :
        cache = None
        cachefile = None
        last_time = None
    else :
        cache, cachefile = init_cache()
        # Figure out the last time we ran feedme.
        # We'll use this for feeds that only update at certain times.
        try :
            statbuf = os.stat(cachefile)
            last_time = statbuf.st_mtime
        except OSError :    # probably the cache is new and got File Not Found
            last_time = 0

    # logfilename = config.get('DEFAULT', 'logfile')
    # if logfilename :
    #     logfilename = feedmeparser.sub_tilde(logfilename)

    # Ignore the config file logfile setting, and log to feeds/LOG.
    # When we're finished, we'll move the file to inside the newly
    # created feed directory.

    feeddir = feedmeparser.sub_tilde(config.get('DEFAULT', 'dir'))
    if not os.path.exists(feeddir):
        os.makedirs(feeddir)
    logfilename = os.path.join(feeddir, 'LOG')

    # Set up a tee to the log file, and redirect stderr there:
    print "teeing output to", logfilename
    stderrsav = sys.stderr
    outputlog = open(logfilename, "w", buffering=1)
    sys.stderr = tee(stderrsav, outputlog)

    try :
        if len(args) == 0 :
            for feedname in sections :
                # This can hang if feedparser hangs parsing the initial RSS.
                # So give the user a chance to ^C out of one feed
                # without stopping the whole run:
                try :
                    get_feed(feedname, config, cache, cachefile,
                             last_time, msglog)
                except KeyboardInterrupt :
                    print >>sys.stderr, "Interrupt! Skipping feed", feedname
                    handle_keyboard_interrupt("Type q to quit, anything else to skip to next feed: ")
                    # We don't actually have to check the return value;
                    # handle_keyboard_interrupt will quit if the user types q.

        #sys.exit(1)
        else :
            for arg in args :
                print >>sys.stderr, 'Getting feed for', arg
                get_feed(arg, config, cache, cachefile, last_time, msglog)

    # This causes a lot of premature exits. Not sure why we end up
    # here rather than in the inner KeyboardInterrupt section.
    except KeyboardInterrupt :
        print >>sys.stderr, "Caught keyboard interrupt at the wrong time!"
        print traceback.format_exc(sys.exc_info()[2])
        #sys.exit(1)
    except OSError, e :
        print >>sys.stderr, "Caught an OSError"
        print >>sys.stderr, e
        #sys.exit(e.errno)

    # Dump any errors we encountered.
    msgs = msglog.get_msgs()
    if msgs :
        print >>sys.stderr, "\n===== Messages ===="
        print >>sys.stderr, msgs.encode('utf-8', 'backslashreplace')
    msgs = msglog.get_errs()
    if msgs :
        print >>sys.stderr, "\n====== Errors ====="
        print >>sys.stderr, msgs.encode('utf-8', 'backslashreplace')

    try :
        # Now we're done. It's time to move the log file into its final place.
        datestr = time.strftime("%m-%d-%a")
        datedir = os.path.join(feeddir, datestr)
        os.rename(logfilename,
                  os.path.join(datedir, 'LOG'))

        # and make a manifest listing all the files we downloaded.
        # This will be used remotely, so we don't want the local
        # path in it; everything is relative to this directory.
        # XXX Temporarily, we'll only do it for me, so Dave doesn't
        # get MANIFEST files cluttering up his directories.
        if 'akkana' in datedir or 'shallowsky' in datedir :
            discardchars = len(datedir)
            print "Discarding", discardchars, "characters"
            manifest = open(os.path.join(datedir, 'MANIFEST'), 'w')
            for root, dirs, files in os.walk(datedir) :
                shortroot = root[discardchars+1:]
                if shortroot :
                    print >>manifest, shortroot + '/'
                for f in files :
                    print >>manifest, posixpath.join(shortroot, f)
    except OSError :
        print >>sys.stderr, "Couldn't move LOG or create MANIFEST"

    # Clean up old directories:
    clean_up(config)

